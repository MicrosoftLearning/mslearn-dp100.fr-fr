{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Détecter et atténuer une partialité dans les modèles\n",
        "\n",
        "Des modèles Machine Learning peuvent incorporer un biais involontaire, ce qui peut conduire à des problèmes d’*impartialité*. Par exemple, un modèle prédisant la probabilité d’un diabète pourrait bien fonctionner pour certains groupes d’âge, mais pas pour d’autres, avec pour effet de soumettre certains patients à des tests superflus, ou de les priver de tests susceptibles de confirmer un diagnostic de diabète.\n",
        "\n",
        "Dans ce notebook, vous allez utiliser le package **Fairlearn** pour analyser un modèle et explorer la disparité des performances de prédiction pour différents sous-ensembles de patients en fonction de l’âge.\n",
        "\n",
        "> **Remarque** : l’intégration avec le package Fairlearn est actuellement en préversion. Il se peut que vous rencontriez des erreurs inattendues.\n",
        "\n",
        "## Important - Considérations relatives à l’impartialité\n",
        "\n",
        "> Ce notebook est conçu comme un exercice pratique pour vous aider à explorer le package Fairlearn et son intégration avec Azure Machine Learning. Cependant, il existe un grand nombre de considérations en matière d’impartialité qu’une organisation ou une équipe de science des données doivent aborder avant d’utiliser les outils. L’impartialité est un défi *socio-technique* complexe qui dépasse la simple exécution d’un outil pour analyser des modèles.\n",
        ">\n",
        "> Microsoft Research a contribué au développement d’une [liste de contrôle de l’impartialité](https://www.microsoft.com/en-us/research/publication/co-designing-checklists-to-understand-organizational-challenges-and-opportunities-around-fairness-in-ai/) qui offre un excellent point de départ pour les discussions importantes qui doivent avoir lieu avant d’écrire une seule ligne de code.\n",
        "\n",
        "## Installer les kits de développement logiciel (SDK) requis\n",
        "\n",
        "Pour utiliser le package Fairlearn avec Azure Machine Learning, vous avez besoin des packages Python Azure Machine Learning et Fairlearn. Exécutez donc la cellule suivante pour vérifier que le package **azureml-contrib-fairness** est installé. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip show azureml-contrib-fairness"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649368422673
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous aurez également besoin du package **fairlearn** proprement dit et du package **raiwidgets** (que Fairlearn utilise pour visualiser des tableaux de bord). Exécutez la cellule suivante pour les installer."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade fairlearn==0.7.0 raiwidgets"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Effectuer l’apprentissage d’un modèle\n",
        "\n",
        "Vous allez commencer par effectuer l’apprentissage d’un modèle de classification pour prédire la probabilité de diabète. En plus de fractionner les données en jeux de caractéristiques et d’étiquettes pour l’apprentissage et les tests, vous allez extraire des caractéristiques *sensibles* utilisées pour définir des sous-populations de données dont vous souhaitez comparer l’impartialité. Dans ce cas, vous allez utiliser la colonne **Age** pour définir deux catégories de patients : les plus de 50 ans et les moins de 51 ans."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# load the diabetes dataset\n",
        "print(\"Loading Data...\")\n",
        "data = pd.read_csv('data/diabetes.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "features = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\n",
        "X, y = data[features].values, data['Diabetic'].values\n",
        "\n",
        "# Get sensitive features\n",
        "S = data[['Age']].astype(int)\n",
        "# Change value to represent age groups\n",
        "S['Age'] = np.where(S.Age > 50, 'Over 50', '50 or younger')\n",
        "\n",
        "# Split data into training set and test set\n",
        "X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(X, y, S, test_size=0.20, random_state=0, stratify=y)\n",
        "\n",
        "# Train a classification model\n",
        "print(\"Training model...\")\n",
        "diabetes_model = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "\n",
        "print(\"Model trained.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant que vous avez effectué l’apprentissage un modèle, vous pouvez utiliser le package Fairlearn afin de comparer le comportement du modèle pour différentes valeurs de caractéristiques sensibles. Dans ce cas, vous allez :\n",
        "\n",
        "- Utiliser la fonction fairlearn **selection_rate** afin de retourner le taux de sélection (pourcentage de prédictions positives) pour la population globale.\n",
        "- Utiliser les fonctions de métriques **scikit-learn** pour calculer les métriques d’exactitude globale, de rappel et de précision.\n",
        "- Utiliser une **MetricFrame** pour calculer le taux de sélection, l’exactitude, le rappel et la précision pour chaque groupe d’âge dans la caractéristique sensible **Age**. Notez qu’une combinaison de fonctions de métriques **fairlearn** et **scikit-learn** est utilisée pour calculer les valeurs de performance."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from fairlearn.metrics import selection_rate, MetricFrame\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "\n",
        "# Get predictions for the witheld test data\n",
        "y_hat = diabetes_model.predict(X_test)\n",
        "\n",
        "# Get overall metrics\n",
        "print(\"Overall Metrics:\")\n",
        "# Get selection rate from fairlearn\n",
        "overall_selection_rate = selection_rate(y_test, y_hat) # Get selection rate from fairlearn\n",
        "print(\"\\tSelection Rate:\", overall_selection_rate)\n",
        "# Get standard metrics from scikit-learn\n",
        "overall_accuracy = accuracy_score(y_test, y_hat)\n",
        "print(\"\\tAccuracy:\", overall_accuracy)\n",
        "overall_recall = recall_score(y_test, y_hat)\n",
        "print(\"\\tRecall:\", overall_recall)\n",
        "overall_precision = precision_score(y_test, y_hat)\n",
        "print(\"\\tPrecision:\", overall_precision)\n",
        "\n",
        "# Get metrics by sensitive group from fairlearn\n",
        "print('\\nMetrics by Group:')\n",
        "metrics = {'selection_rate': selection_rate,\n",
        "           'accuracy': accuracy_score,\n",
        "           'recall': recall_score,\n",
        "           'precision': precision_score}\n",
        "\n",
        "group_metrics = MetricFrame(metrics=metrics,\n",
        "                             y_true=y_test,\n",
        "                             y_pred=y_hat,\n",
        "                             sensitive_features=S_test['Age'])\n",
        "\n",
        "print(group_metrics.by_group)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ces métriques devraient vous permettre de discerner qu’une plus grande proportion des patients plus âgés sont prédits comme étant diabétiques. La *précision* devrait être plus ou moins égale pour les deux groupes, mais un examen plus approfondi des métriques *précision* et *rappel* révèle une certaine disparité dans la qualité de prédiction du modèle pour chaque groupe d’âge.\n",
        "\n",
        "Dans ce scénario, examinez la métrique *rappel*. Cette métrique indique la proportion de cas positifs correctement identifiés par le modèle. En d’autres termes, de tous les patients réellement diabétiques, combien le modèle en a-t-il trouvé ? Le modèle est plus performant pour les patients âgés que pour les patients jeunes.\n",
        "\n",
        "Il est souvent plus facile de comparer les métriques visuellement. Pour ce faire, vous allez utiliser le tableau de bord d’impartialité Fairlearn :\n",
        "\n",
        "1. Exécutez la cellule ci-dessous pour générer un tableau de bord à partir du modèle que vous avez créé précédemment.\n",
        "2. Une fois le widget affiché, utilisez le lien **Prise en main** pour commencer à configurer votre visualisation.\n",
        "3. Sélectionnez les caractéristiques sensibles que vous souhaitez comparer (en l’occurrence, il n’y en a qu’une : **Age**).\n",
        "4. Sélectionnez la métrique de performance du modèle que vous souhaitez comparer (en l’occurrence, comme il s’agit d’un modèle de classification binaire, les options sont *Exactitude*, *Exactitude équilibrée*, *Précision* et *Rappel*). Commencez par **Rappel**.\n",
        "5. Sélectionnez le type de comparaison d’impartialité que vous souhaitez afficher. Commencez par **Différence de parité démographique**.\n",
        "6. Affichez les graphiques du tableau de bord :\n",
        "    - **Taux de sélection** : comparaison du nombre de cas positifs par sous-population.\n",
        "    - **Taux de faux positifs et de faux négatifs** : comparaison de la métrique de performance sélectionnée est comparée pour les sous-populations, à savoir *sous-prédiction* (faux négatifs) et *sur-prédiction* (faux positifs).\n",
        "7. Modifiez la configuration pour comparer les prédictions en fonction de différentes métriques de performance et d’impartialité."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from raiwidgets import FairnessDashboard\n",
        "\n",
        "# View this model in Fairlearn's fairness dashboard, and see the disparities which appear:\n",
        "FairnessDashboard(sensitive_features=S_test,\n",
        "                   y_true=y_test,\n",
        "                   y_pred={\"diabetes_model\": diabetes_model.predict(X_test)})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les résultats indiquent un taux de sélection sensiblement plus élevé pour les patients de plus de 50 ans que pour les patients plus jeunes. Toutefois, en réalité, l’âge étant un véritable vecteur de diabète, vous pouvez vous attendre à davantage de cas positifs chez les patients plus âgés.\n",
        "\n",
        "Si nous basons la performance du modèle sur l’*exactitude* (autrement dit, le pourcentage de bonnes prédictions du modèle), il semble fonctionner de manière plus ou moins égale pour les deux sous-populations. Toutefois, selon les métriques de *précision* et de *rappel*, le modèle a tendance à mieux fonctionner pour les patients âgés de plus de 50 ans.\n",
        "\n",
        "Voyons ce qui se passe si nous excluons la caractéristique **Age** lors de l’apprentissage du modèle."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and labels\n",
        "ageless = features.copy()\n",
        "ageless.remove('Age')\n",
        "X2, y2 = data[ageless].values, data['Diabetic'].values\n",
        "\n",
        "# Split data into training set and test set\n",
        "X_train2, X_test2, y_train2, y_test2, S_train2, S_test2 = train_test_split(X2, y2, S, test_size=0.20, random_state=0, stratify=y2)\n",
        "\n",
        "# Train a classification model\n",
        "print(\"Training model...\")\n",
        "ageless_model = DecisionTreeClassifier().fit(X_train2, y_train2)\n",
        "print(\"Model trained.\")\n",
        "\n",
        "# View this model in Fairlearn's fairness dashboard, and see the disparities which appear:\n",
        "FairnessDashboard(sensitive_features=S_test2,\n",
        "                   y_true=y_test2,\n",
        "                   y_pred={\"ageless_diabetes_model\": ageless_model.predict(X_test2)})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explorez le modèle dans le tableau de bord.\n",
        "\n",
        "Lorsque vous examinez la métrique *rappel*, notez que la disparité a diminué, mais que le rappel global a également diminué car le modèle sous-prédit désormais de manière significative les cas positifs pour les patients plus âgés. Même si la caractéristique **Age** n’a pas été utilisée dans l’apprentissage, le modèle présente toujours une certaine disparité dans la qualité de ses prédictions pour les patients plus âgés et plus jeunes.\n",
        "\n",
        "Dans ce scénario, la simple suppression de la caractéristique **Age** réduit légèrement la disparité dans la métrique de *rappel*, mais l’augmente dans les métriques de *précision* et d’*exactitude*. Cela souligne l’une des principales difficultés de l’application de l’impartialité aux modèles d’apprentissage automatique. Vous devez comprendre clairement ce que signifie l’« impartialité » dans un contexte particulier, et l’optimiser en conséquence.\n",
        "\n",
        "## Inscrire le modèle et charger les données du tableau de bord dans votre espace de travail\n",
        "\n",
        "Vous avez effectué l’apprentissage du modèle et examiné le tableau de bord localement dans ce notebook. Toutefois, il pourrait être utile d’inscrire le modèle dans votre espace de travail Azure Machine Learning, et de créer un essai pour enregistrer les données du tableau de bord afin de pouvoir suivre et partager votre analyse d’impartialité.\n",
        "\n",
        "Commençons par inscrire le modèle d’origine (qui incluait la caractéristique **Age**).\n",
        "\n",
        "> **Remarque** : si vous n’avez pas encore établi de session authentifiée avec votre abonnement Azure, vous serez invité à vous authentifier en cliquant sur un lien, en saisissant un code d’authentification et en vous connectant à Azure."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment, Model\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Load the Azure ML workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to work with', ws.name)\n",
        "\n",
        "# Save the trained model\n",
        "model_file = 'diabetes_model.pkl'\n",
        "joblib.dump(value=diabetes_model, filename=model_file)\n",
        "\n",
        "# Register the model\n",
        "print('Registering model...')\n",
        "registered_model = Model.register(model_path=model_file,\n",
        "                                  model_name='diabetes_classifier',\n",
        "                                  workspace=ws)\n",
        "model_id= registered_model.id\n",
        "\n",
        "\n",
        "print('Model registered.', model_id)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez maintenant utiliser le package FairLearn pour créer des jeux de métriques de groupe de classification binaire pour un ou plusieurs modèles, et utiliser un essai Azure Machine Learning pour charger les métriques.\n",
        "\n",
        "> ** Remarque ** : cette opération peut prendre un certain temps et entraîner l’affichage de messages d’avertissement (que vous pouvez ignorer). Une fois l’essai terminé, les données du tableau de bord seront téléchargées et affichées pour permettre de vérifier que leur chargement a abouti."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from fairlearn.metrics._group_metric_set import _create_group_metric_set\n",
        "from azureml.contrib.fairness import upload_dashboard_dictionary, download_dashboard_by_upload_id\n",
        "\n",
        "#  Create a dictionary of model(s) you want to assess for fairness \n",
        "sf = { 'Age': S_test.Age}\n",
        "ys_pred = { model_id:diabetes_model.predict(X_test) }\n",
        "dash_dict = _create_group_metric_set(y_true=y_test,\n",
        "                                    predictions=ys_pred,\n",
        "                                    sensitive_features=sf,\n",
        "                                    prediction_type='binary_classification')\n",
        "\n",
        "exp = Experiment(ws, 'mslearn-diabetes-fairness')\n",
        "print(exp)\n",
        "\n",
        "run = exp.start_logging()\n",
        "\n",
        "# Upload the dashboard to Azure Machine Learning\n",
        "try:\n",
        "    dashboard_title = \"Fairness insights of Diabetes Classifier\"\n",
        "    upload_id = upload_dashboard_dictionary(run,\n",
        "                                            dash_dict,\n",
        "                                            dashboard_name=dashboard_title)\n",
        "    print(\"\\nUploaded to id: {0}\\n\".format(upload_id))\n",
        "\n",
        "    # To test the dashboard, you can download it\n",
        "    downloaded_dict = download_dashboard_by_upload_id(run, upload_id)\n",
        "    print(downloaded_dict)\n",
        "finally:\n",
        "    run.complete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code précédent téléchargeait les métriques générées dans l’essai juste pour confirmer qu’elles avaient abouti. Le véritable avantage du chargement des métriques dans un essai est qu’il vous permet d’afficher le tableau de bord FairLearn dans Azure Machine Learning studio.\n",
        "\n",
        "Exécutez la cellule ci-dessous pour voir les détails de l’essai, puis cliquez sur le lien **Afficher les détails de l’exécution** dans le widget pour voir l’exécution dans Azure Machine Learning studio. Ensuite, affichez l’onglet **Impartialité** de l’exécution de l’essai afin de voir sur le tableau de bord l’ID d’impartialité affecté aux métriques que vous avez chargées, qui se comporte de la même façon que le widget que vous avez affiché précédemment dans ce notebook."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "\n",
        "RunDetails(run).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez également trouver le tableau de bord d’impartialité en sélectionnant un modèle dans la page **Modèles** d’Azure Machine Learning studio et en examinant son onglet **Impartialité**. Cela permet à votre organisation de conserver un journal d’analyse d’impartialité pour les modèles dont vous effectuez l’apprentissage et que vous inscrivez."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Atténuer la partialité dans le modèle\n",
        "\n",
        "Maintenant que vous avez analysé le modèle en lien avec l’impartialité, vous pouvez utiliser l’une des techniques d’*atténuation* prises en charge par le package FairLearn afin de trouver un modèle qui équilibre les performances prédictives et l’impartialité.\n",
        "\n",
        "Dans cet exercice, vous allez utiliser la caractéristique **GridSearch**, qui effectue l’apprentissage de plusieurs modèles pour tenter de réduire la disparité des performances prédictives pour les caractéristiques sensibles dans le jeu de données (en l’occurrence, les groupes d’âge). Vous allez optimiser les modèles en appliquant la contrainte de parité **EqualizedOdds**, qui veille à ce que les modèles présentant des taux de vrais et faux positifs similaires pour chaque regroupement de caractéristiques sensibles. \n",
        "\n",
        "> *Cela peut prendre un certain temps*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from fairlearn.reductions import GridSearch, EqualizedOdds\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "print('Finding mitigated models...')\n",
        "\n",
        "# Train multiple models\n",
        "sweep = GridSearch(DecisionTreeClassifier(),\n",
        "                   constraints=EqualizedOdds(),\n",
        "                   grid_size=20)\n",
        "\n",
        "sweep.fit(X_train, y_train, sensitive_features=S_train.Age)\n",
        "models = sweep.predictors_\n",
        "\n",
        "# Save the models and get predictions from them (plus the original unmitigated one for comparison)\n",
        "model_dir = 'mitigated_models'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_name = 'diabetes_unmitigated'\n",
        "print(model_name)\n",
        "joblib.dump(value=diabetes_model, filename=os.path.join(model_dir, '{0}.pkl'.format(model_name)))\n",
        "predictions = {model_name: diabetes_model.predict(X_test)}\n",
        "i = 0\n",
        "for model in models:\n",
        "    i += 1\n",
        "    model_name = 'diabetes_mitigated_{0}'.format(i)\n",
        "    print(model_name)\n",
        "    joblib.dump(value=model, filename=os.path.join(model_dir, '{0}.pkl'.format(model_name)))\n",
        "    predictions[model_name] = model.predict(X_test)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez maintenant utiliser le tableau de bord FairLearn pour comparer les modèles atténués :\n",
        "\n",
        "Exécutez la cellule suivante, puis utilisez l’Assistant pour visualiser la caractéristique **Age** par **Rappel**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "FairnessDashboard(sensitive_features=S_test,\n",
        "                   y_true=y_test,\n",
        "                   y_pred=predictions)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les modèles sont affichés sur un nuage de points. Vous pouvez comparer les modèles en mesurant la disparité des prédictions (en d’autres termes, le taux de sélection) ou la disparité de la métrique de performance sélectionnée (en l’occurrence, *rappel*). Dans ce scénario, nous nous attendons à une disparité des taux de sélection (car nous savons que l’âge *est* un facteur de diabète, avec davantage de cas positifs dans le groupe plus âgé). Nous nous intéressons à la disparité des performances prédictives. Sélectionnez donc l’option pour mesurer la **Disparité de rappel**.\n",
        "\n",
        "Le graphique montre des clusters de modèles avec la métrique globale *rappel* sur l’axe X, et la disparité de rappel sur l’axe Y. Par conséquent, le modèle idéal (avec rappel élevé et faible disparité) se trouve dans l’angle inférieur droit du tracé. Vous pouvez choisir le bon équilibre entre performances prédictives et impartialité pour vos besoins particuliers, et sélectionner un modèle approprié pour afficher ses détails.\n",
        "\n",
        "Il est important de souligner que l’application d’une atténuation d’impartialité à un modèle est un compromis entre les performances prédictives globales et la disparité parmi les groupes de caractéristiques sensibles. En général, vous devez sacrifier un peu des performances prédictives globales pour vous assurer que le modèle prédise de façon impartiale pour tous les segments de la population.\n",
        "\n",
        "> **Remarque** : l’affichage de la métrique * précision* peut entraîner l’affichage d’un avertissement indiquant que la précision est définie sur 0,0 à défaut d’échantillon prédit. Vous pouvez l’ignorer.\n",
        "\n",
        "## Charger les métriques du tableau de bord d’atténuation sur Azure Machine Learning\n",
        "\n",
        "Comme précédemment, il se peut que vous vouliez conserver une trace de votre expérimentation d’atténuation. Pour ce faire, vous pouvez :\n",
        "\n",
        "1. Inscrire les modèles trouvés par le processus GridSearch.\n",
        "2. Calculer les métriques de performances et de disparité pour les modèles.\n",
        "3. Charger les métriques dans un essai Azure Machine Learning."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the models\n",
        "registered_model_predictions = dict()\n",
        "for model_name, prediction_data in predictions.items():\n",
        "    model_file = os.path.join(model_dir, model_name + \".pkl\")\n",
        "    registered_model = Model.register(model_path=model_file,\n",
        "                                      model_name=model_name,\n",
        "                                      workspace=ws)\n",
        "    registered_model_predictions[registered_model.id] = prediction_data\n",
        "\n",
        "#  Create a group metric set for binary classification based on the Age feature for all of the models\n",
        "sf = { 'Age': S_test.Age}\n",
        "dash_dict = _create_group_metric_set(y_true=y_test,\n",
        "                                     predictions=registered_model_predictions,\n",
        "                                     sensitive_features=sf,\n",
        "                                     prediction_type='binary_classification')\n",
        "\n",
        "exp = Experiment(ws, \"mslearn-diabetes-fairness\")\n",
        "print(exp)\n",
        "\n",
        "run = exp.start_logging()\n",
        "RunDetails(run).show()\n",
        "\n",
        "# Upload the dashboard to Azure Machine Learning\n",
        "try:\n",
        "    dashboard_title = \"Fairness Comparison of Diabetes Models\"\n",
        "    upload_id = upload_dashboard_dictionary(run,\n",
        "                                            dash_dict,\n",
        "                                            dashboard_name=dashboard_title)\n",
        "    print(\"\\nUploaded to id: {0}\\n\".format(upload_id))\n",
        "finally:\n",
        "    run.complete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Remarque** : il se peut qu’un avertissement indiquant que la précision est définie sur 0,0 à défaut d’échantillon prédit. Vous pouvez l’ignorer.\n",
        "\n",
        "\n",
        "Une fois l’essai terminé, cliquez sur le lien **Afficher les détails de l’exécution** dans le widget pour voir l’exécution dans Azure Machine Learning studio (il se peut que vous deviez faire défiler au-delà la sortie initiale pour voir le widget), ainsi que le tableau de bord FairLearn sous l’onglet **Impartialité**."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}