{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exécuter des essais\n",
        "\n",
        "Vous pouvez utiliser le Kit de développement logiciel (SDK) Azure Machine Learning pour exécuter des essais de code qui journalisent des métriques et génèrent des sorties. Cela est au cœur de la plupart des opérations d’apprentissage automatique dans Azure Machine Learning.\n",
        "\n",
        "## Vous connecter à votre espace de travail\n",
        "\n",
        "L’ensemble des essais et des ressources associées sont gérés dans votre espace de travail Azure Machine Learning. Dans la plupart des cas, vous devriez stocker la configuration de l’espace de travail dans un fichier de configuration JSON. Cela facilite la reconnexion sans qu’il soit nécessaire de se souvenir de détails comme l’ID de votre abonnement Azure. Vous pouvez télécharger le fichier de configuration JSON à partir du volet de votre espace de travail dans le portail Azure mais, si vous utilisez une instance de calcul dans votre espace de travail, le fichier de configuration a déjà été téléchargé dans le dossier racine.\n",
        "\n",
        "Le code ci-dessous utilise le fichier de configuration pour se connecter à votre espace de travail.\n",
        "\n",
        "> **Remarque** : si vous n’avez pas encore établi de session authentifiée avec votre abonnement Azure, vous serez invité à vous authentifier en cliquant sur un lien, en saisissant un code d’authentification et en vous connectant à Azure."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363615859
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exécuter un essai\n",
        "\n",
        "L’une des tâches les plus fondamentales que les scientifiques de données doivent effectuer consiste à créer et exécuter des essais qui traitent et analysent des données. Dans cet exercice, vous allez découvrir comment utiliser un *essai* Azure ML pour exécuter du code Python et enregistrer des valeurs extraites de données. Dans ce cas, vous allez utiliser un simple jeu de données contenant des détails de patients testés pour le diabète. Vous allez exécuter un essai pour explorer les données, extrayant des statistiques, des visualisations et des échantillons de données. Le code que vous allez utiliser est essentiellement du code Python assez générique, comme celui que vous pourriez utiliser dans tout processus d’exploration de données. Toutefois, moyennant l’ajout de quelques lignes, le code utilise un *essai* Azure ML pour journaliser les détails de l’exécution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "# Create an Azure ML experiment in your workspace\n",
        "experiment = Experiment(workspace=ws, name=\"mslearn-diabetes\")\n",
        "\n",
        "# Start logging data from the experiment, obtaining a reference to the experiment run\n",
        "run = experiment.start_logging()\n",
        "print(\"Starting experiment:\", experiment.name)\n",
        "\n",
        "# load the data from a local file\n",
        "data = pd.read_csv('data/diabetes.csv')\n",
        "\n",
        "# Count the rows and log the result\n",
        "row_count = (len(data))\n",
        "run.log('observations', row_count)\n",
        "print('Analyzing {} rows of data'.format(row_count))\n",
        "\n",
        "# Plot and log the count of diabetic vs non-diabetic patients\n",
        "diabetic_counts = data['Diabetic'].value_counts()\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "ax = fig.gca()    \n",
        "diabetic_counts.plot.bar(ax = ax) \n",
        "ax.set_title('Patients with Diabetes') \n",
        "ax.set_xlabel('Diagnosis') \n",
        "ax.set_ylabel('Patients')\n",
        "plt.show()\n",
        "run.log_image(name='label distribution', plot=fig)\n",
        "\n",
        "# log distinct pregnancy counts\n",
        "pregnancies = data.Pregnancies.unique()\n",
        "run.log_list('pregnancy categories', pregnancies)\n",
        "\n",
        "# Log summary statistics for numeric columns\n",
        "med_columns = ['PlasmaGlucose', 'DiastolicBloodPressure', 'TricepsThickness', 'SerumInsulin', 'BMI']\n",
        "summary_stats = data[med_columns].describe().to_dict()\n",
        "for col in summary_stats:\n",
        "    keys = list(summary_stats[col].keys())\n",
        "    values = list(summary_stats[col].values())\n",
        "    for index in range(len(keys)):\n",
        "        run.log_row(col, stat=keys[index], value = values[index])\n",
        "        \n",
        "# Save a sample of the data and upload it to the experiment output\n",
        "data.sample(100).to_csv('sample.csv', index=False, header=True)\n",
        "run.upload_file(name='outputs/sample.csv', path_or_stream='./sample.csv')\n",
        "\n",
        "# Complete the run\n",
        "run.complete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "gather": {
          "logged": 1649363619696
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Afficher les détails de l’exécution\n",
        "\n",
        "Dans Jupyter Notebook, vous pouvez utiliser le widget **RunDetails** pour visualiser les détails de l’exécution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "\n",
        "RunDetails(run).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363626246
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Afficher plus de détails dans Azure Machine Learning studio\n",
        "\n",
        "Notez que le widget **RunDetails** contient un lien pour **afficher les détails de l’exécution** dans Azure Machine Learning studio. Cliquez dessus pour ouvrir un nouvel onglet de navigateur contenant les détails de l’exécution (vous pouvez aussi simplement ouvrir [Azure Machine Learning studio](https://ml.azure.com) et rechercher l’exécution sur la page **Essais**). Lorsque vous affichez l’exécution dans Azure Machine Learning studio, notez ce qui suit :\n",
        "\n",
        "- L’onglet **Détails** contient les propriétés générales de l’exécution de l’essai.\n",
        "- L’onglet **Métriques** vous permet de sélectionner des métriques journalisées et de les afficher sous forme de tables ou de graphiques.\n",
        "- L’onglet **Images** vous permet de sélectionner et d’afficher des images ou tracés journalisés dans l’essai (en l’occurrence, le tracé *Distribution d’étiquette*).\n",
        "- L’onglet **Exécutions enfants** répertorie toutes les exécutions enfants (il n’y en a aucune dans cet essai).\n",
        "- L’onglet **Sorties + Journaux** affiche la sortie ou les fichiers journaux générés par l’essai.\n",
        "- L’onglet **Instantané** contient tous les fichiers du dossier dans lequel le code d’essai a été exécuté (en l’occurrence, tout ce qui se trouve dans le même dossier que ce notebook).\n",
        "- L’onglet **Explications** est utilisé pour afficher les explications de modèle générées par l’essai (en l’occurrence, il n’y en a aucune).\n",
        "- L’onglet **Impartialité** est utilisé pour visualiser les écarts de performances prédictives qui vous aident à évaluer l’impartialité des modèles d’apprentissage (en l’occurrence, il n’y en a aucune)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Récupérer les détails de l’essai à l’aide du Kit de développement logiciel (SDK)\n",
        "\n",
        "La variable **run** dans le code que vous avez exécuté précédemment est une instance d’un objet **Run**, qui est une référence à une exécution individuelle d’un essai dans Azure Machine Learning. Vous pouvez utiliser cette référence pour obtenir des informations sur l’exécution et ses sorties :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Get logged metrics\n",
        "print(\"Metrics:\")\n",
        "metrics = run.get_metrics()\n",
        "for metric_name in metrics:\n",
        "    print(metric_name, \":\", metrics[metric_name])\n",
        "\n",
        "# Get output files\n",
        "print(\"\\nFiles:\")\n",
        "files = run.get_file_names()\n",
        "for file in files:\n",
        "    print(file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363632957
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez télécharger les fichiers générés par l’essai, soit individuellement à l’aide de la méthode **download_file**, soit à l’aide de la méthode **download_files** pour récupérer plusieurs fichiers. Le code suivant télécharge tous les fichiers dans le dossier **output** de l’exécution :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "download_folder = 'downloaded-files'\n",
        "\n",
        "# Download files in the \"outputs\" folder\n",
        "run.download_files(prefix='outputs', output_directory=download_folder)\n",
        "\n",
        "# Verify the files have been downloaded\n",
        "for root, directories, filenames in os.walk(download_folder): \n",
        "    for filename in filenames:  \n",
        "        print (os.path.join(root,filename))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363635892
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si vous devez résoudre des problèmes liés à l’exécution de l’essai, vous pouvez utiliser la méthode **get_details** pour récupérer des détails de base sur l’exécution, ou la méthode **get_details_with_logs** pour récupérer les détails de l’exécution, ainsi que le contenu des fichiers journaux générés pendant l’exécution :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run.get_details_with_logs()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363638967
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notez que les détails incluent des informations sur la cible de calcul sur laquelle l’essai a été exécuté, la date et l’heure de début et de fin de celui-ci. En outre, étant donné que le notebook contenant le code d’essai (celui-ci) se trouve dans un dépôt Git cloné, les détails sur le dépôt, la branche et l’état sont enregistrés dans l’historique des exécutions.\n",
        "\n",
        "Dans ce cas, notez que l’entrée **logFiles** dans les détails indique qu’aucun fichier journal n’a été généré. C’est typique d’un essai inclus comme celui que vous avez exécuté, mais les choses deviennent plus intéressantes lorsque vous exécutez un script en tant qu’expérience, ce que nous allons examiner maintenant."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exécuter un script d’essai\n",
        "\n",
        "Dans l’exemple précédent, vous avez exécuté un essai inclus dans ce notebook. Une solution plus souple consiste à créer un script distinct pour l’essai, et à le stocker dans un dossier avec tous les autres fichiers dont il a besoin, puis à utiliser Azure ML pour exécuter l’essai basé sur le script dans le dossier.\n",
        "\n",
        "Tout d’abord, créons un dossier pour les fichiers d’essai, et copions-y les données :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "# Create a folder for the experiment files\n",
        "folder_name = 'diabetes-experiment-files'\n",
        "experiment_folder = './' + folder_name\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# Copy the data file into the experiment folder\n",
        "shutil.copy('data/diabetes.csv', os.path.join(folder_name, \"diabetes.csv\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363641423
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons maintenant créer un script Python contenant le code de notre essai, puis l’enregistrer dans le dossier d’essai.\n",
        "\n",
        "> **Remarque** : l’exécution de la cellule suivante *crée* simplement le fichier de script, mais ne l’exécute pas."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $folder_name/diabetes_experiment.py\n",
        "from azureml.core import Run\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Get the experiment run context\n",
        "run = Run.get_context()\n",
        "\n",
        "# load the diabetes dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Count the rows and log the result\n",
        "row_count = (len(data))\n",
        "run.log('observations', row_count)\n",
        "print('Analyzing {} rows of data'.format(row_count))\n",
        "\n",
        "# Count and log the label counts\n",
        "diabetic_counts = data['Diabetic'].value_counts()\n",
        "print(diabetic_counts)\n",
        "for k, v in diabetic_counts.items():\n",
        "    run.log('Label:' + str(k), v)\n",
        "      \n",
        "# Save a sample of the data in the outputs folder (which gets uploaded automatically)\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "data.sample(100).to_csv(\"outputs/sample.csv\", index=False, header=True)\n",
        "\n",
        "# Complete the run\n",
        "run.complete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code est une version simplifiée du code inclus utilisé avant. Cependant, notez les points suivants :\n",
        "- Il utilise la méthode « Run.get_context() » pour récupérer le contexte d’exécution de l’essai lors de l’exécution du script.\n",
        "- Il charge les données sur le diabète à partir du dossier dans lequel se trouve le script.\n",
        "- Il crée un dossier nommé **outputs** dans lequel il écrit l’exemple de fichier. Ce dossier est chargé automatiquement dans l’exécution de l’essai"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant, vous êtes presque prêt à exécuter l’essai. Pour exécuter le script, vous devez créer une **ScriptRunConfig** qui identifie le fichier script Python à exécuter dans l’essai, puis exécuter un essai basé sur celui-ci.\n",
        "\n",
        "> **Remarque** : la ScriptRunConfig détermine également la cible de calcul et l’environnement Python. Dans ce cas, l’environnement Python est défini de façon à inclure des packages Conda et pip, mais la cible de calcul est omise. Par conséquent, l’instance de calcul locale par défaut sera utilisée.\n",
        "\n",
        "La cellule suivante configure et soumet l’essai basé sur un script."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
        "from azureml.core.runconfig import DockerConfiguration\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "# Create a Python environment for the experiment (from a .yml file)\n",
        "env = Environment.from_conda_specification(\"experiment_env\", \"environment.yml\")\n",
        "\n",
        "# Create a script config\n",
        "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
        "                                script='diabetes_experiment.py',\n",
        "                                environment=env,\n",
        "                                docker_runtime_config=DockerConfiguration(use_docker=True))\n",
        "\n",
        "# submit the experiment\n",
        "experiment = Experiment(workspace=ws, name='mslearn-diabetes')\n",
        "run = experiment.submit(config=script_config)\n",
        "RunDetails(run).show()\n",
        "run.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363661425
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comme précédemment, vous pouvez utiliser le widget ou le lien vers l’essai dans [Azure Machine Learning studio](https://ml.azure.com) pour afficher les sorties générées par l’essai, ainsi qu’écrire du code pour récupérer les métriques et fichiers qu’il a générés :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get logged metrics\n",
        "metrics = run.get_metrics()\n",
        "for key in metrics.keys():\n",
        "        print(key, metrics.get(key))\n",
        "print('\\n')\n",
        "for file in run.get_file_names():\n",
        "    print(file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363665408
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notez que, cette fois, l’exécution a généré des fichiers journaux. Vous pouvez les afficher dans le widget, ou utiliser la méthode **get_details_with_logs** comme nous l’avons fait précédemment mais, cette fois, la sortie inclura les données du journal."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run.get_details_with_logs()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363668206
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bien que vous puissiez afficher les détails du journal dans la sortie ci-dessus, il est généralement plus facile de télécharger les fichiers journaux et de les afficher dans un éditeur de texte."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "log_folder = 'downloaded-logs'\n",
        "\n",
        "# Download all files\n",
        "run.get_all_logs(destination=log_folder)\n",
        "\n",
        "# Verify the files have been downloaded\n",
        "for root, directories, filenames in os.walk(log_folder): \n",
        "    for filename in filenames:  \n",
        "        print (os.path.join(root,filename))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363670778
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Afficher l’historique des exécutions de l’essai\n",
        "\n",
        "Maintenant que vous avez exécuté le même essai plusieurs fois, vous pouvez afficher l’historique des exécutions dans [Azure Machine Learning studio](https://ml.azure.com) et explorer chaque exécution journalisée. Vous pouvez également récupérer un essai par son nom à partir de l’espace de travail, et itérer dans ses exécutions à l’aide du Kit de développement logiciel (SDK) :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment, Run\n",
        "\n",
        "diabetes_experiment = ws.experiments['mslearn-diabetes']\n",
        "for logged_run in diabetes_experiment.get_runs():\n",
        "    print('Run ID:', logged_run.id)\n",
        "    metrics = logged_run.get_metrics()\n",
        "    for key in metrics.keys():\n",
        "        print('-', key, metrics.get(key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363706798
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utiliser MLflow\n",
        "\n",
        "MLflow est une plateforme open source pour la gestion des processus d’apprentissage automatique. Il est généralement (mais pas exclusivement) utilisé dans des environnements Databricks pour coordonner des essais et suivre des métriques. Dans des essais Azure Machine Learning, vous pouvez utiliser MLflow pour suivre des métriques, en guise d’alternative à la fonctionnalité de journal native.\n",
        "\n",
        "Pour tirer parti de cette fonctionnalité, vous allez avoir besoin du package **azureml-mlflow**. Vérifions donc qu’il est installé."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip show azureml-mlflow"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363713535
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utiliser MLflow avec un essai inclus\n",
        "\n",
        "Pour utiliser MLflow afin de suivre des métriques pour un essai inclus, vous devez définir l’*URI de suivi* MLflow sur l’espace de travail dans lequel l’essai est exécuté. Cela vous permet d’utiliser les méthodes de suivi **mlflow** pour journaliser des données pendant l’exécution de l’essai."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "\n",
        "# Set the MLflow tracking URI to the workspace\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "\n",
        "# Create an Azure ML experiment in your workspace\n",
        "experiment = Experiment(workspace=ws, name='mslearn-diabetes-mlflow')\n",
        "mlflow.set_experiment(experiment.name)\n",
        "\n",
        "# start the MLflow experiment\n",
        "with mlflow.start_run():\n",
        "    \n",
        "    print(\"Starting experiment:\", experiment.name)\n",
        "    \n",
        "    # Load data\n",
        "    data = pd.read_csv('data/diabetes.csv')\n",
        "\n",
        "    # Count the rows and log the result\n",
        "    row_count = (len(data))\n",
        "    mlflow.log_metric('observations', row_count)\n",
        "    print(\"Run complete\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363717485
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examinons à présent les métriques journalisées lors de l’exécution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the latest run of the experiment\n",
        "run = list(experiment.get_runs())[0]\n",
        "\n",
        "# Get logged metrics\n",
        "print(\"\\nMetrics:\")\n",
        "metrics = run.get_metrics()\n",
        "for key in metrics.keys():\n",
        "        print(key, metrics.get(key))\n",
        "    \n",
        "# Get a link to the experiment in Azure ML studio   \n",
        "experiment_url = experiment.get_portal_url()\n",
        "print('See details at', experiment_url)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363720202
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Après avoir exécuté le code ci-dessus, vous pouvez utiliser le lien affiché pour voir l’essai dans Azure Machine Learning studio. Sélectionnez ensuite la dernière exécution de l’essai et affichez son onglet **Métriques** pour voir la métrique journalisée.\n",
        "\n",
        "### Utiliser MLflow dans un script d’essai\n",
        "\n",
        "Vous pouvez également utiliser MLflow pour suivre des métriques dans un script d’essai.\n",
        "\n",
        "Exécutez les deux cellules suivantes pour créer un dossier et un script pour un essai utilisant MLflow."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "# Create a folder for the experiment files\n",
        "folder_name = 'mlflow-experiment-files'\n",
        "experiment_folder = './' + folder_name\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# Copy the data file into the experiment folder\n",
        "shutil.copy('data/diabetes.csv', os.path.join(folder_name, \"diabetes.csv\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363722917
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $folder_name/mlflow_diabetes.py\n",
        "from azureml.core import Run\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "\n",
        "\n",
        "# start the MLflow experiment\n",
        "with mlflow.start_run():\n",
        "       \n",
        "    # Load data\n",
        "    data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "    # Count the rows and log the result\n",
        "    row_count = (len(data))\n",
        "    print('observations:', row_count)\n",
        "    mlflow.log_metric('observations', row_count)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lorsque vous utilisez le suivi MLflow dans un script d’essai Azure ML, l’URI de suivi MLflow est défini automatiquement au démarrage de l’exécution de l’essai. Toutefois, l’environnement dans lequel le script doit être exécuté doit inclure les packages **mlflow** requis."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
        "from azureml.core.runconfig import DockerConfiguration\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "\n",
        "# Create a Python environment for the experiment (from a .yml file)\n",
        "env = Environment.from_conda_specification(\"experiment_env\", \"environment.yml\")\n",
        "\n",
        "# Create a script config\n",
        "script_mlflow = ScriptRunConfig(source_directory=experiment_folder,\n",
        "                                script='mlflow_diabetes.py',\n",
        "                                environment=env,\n",
        "                                docker_runtime_config=DockerConfiguration(use_docker=True)) \n",
        "\n",
        "# submit the experiment\n",
        "experiment = Experiment(workspace=ws, name='mslearn-diabetes-mlflow')\n",
        "run = experiment.submit(config=script_mlflow)\n",
        "RunDetails(run).show()\n",
        "run.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "gather": {
          "logged": 1649363740151
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comme d’habitude, vous pouvez vous procurer les métriques journalisées de l’exécution de l’essai une fois celui-ci terminé."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get logged metrics\n",
        "metrics = run.get_metrics()\n",
        "for key in metrics.keys():\n",
        "        print(key, metrics.get(key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1649363744091
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Informations supplémentaires** : pour en savoir plus sur l’exécution d’essais, consultez [cette rubrique](https://docs.microsoft.com/azure/machine-learning/how-to-manage-runs) dans la documentation d’Azure ML. Pour plus d’informations sur la façon de journaliser les métriques d’une exécution, consultez [cette rubrique](https://docs.microsoft.com/azure/machine-learning/how-to-track-experiments). Pour plus d’informations sur l’intégration d’essais Azure ML avec MLflow, consultez [cette rubrique](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow)."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}